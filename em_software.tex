\chapter{Software in cryo-ET}\label{software}

Due to the rapid growth of cryo-ET, the software ecosystem as a whole is fragmented; conventions are either not established, or appear on a first-come-first-served basis, with little consensus or community discussion.
Meanwhile, the most established pipelines use big monolithic applications, developed with streamlined workflows in mind and little attention to code reusability.

\localtableofcontents

\section{Monolithic vs Modular: an academia paradox}

In academia, large publications with splashy headlines can make or break career paths, rewarding speed, competition, and secrecy; unfortunately, this is not conductive to the development of good software practices.

Researchers regularly develop ad hoc solutions, lacking the time and resources to release a maintainable tool, leading to software that can hardly be adapted even to similar problems.
These researchers are also typically PhD students that will soon move to a different group, work on different projects, and won't have any more time to dedicate to their old software.
It's rarely in their best interest to keep working on software that won't give them any further publications, so there's little incentive to begin development with modularity and maintainability in mind. 
All this leads to regular wheel-reinventing, and abandonware full of great ideas but hard to reuse in any capacity for future work.

In the rare success stories where software becomes adopted widely enough to achieve long term maintenance, it's usually in the form of inscrutable monolithic applications developed by a single group or person with little community collaboration on the development.
There are of course exceptions, but for young researchers it's generally a safer bet to go the "develop and publish" route than the "design and maintain" one; of course, most research developers struggle with this tension, wishing for their work to be reused and improved, but unable to allocate the time and resources required to do so.

\subsection{Metadata wrangling}\label{metadata}

A common symptom of such an ecosystem is that significant time and effort is lost in dealing with metadata when setting up a cryo-ET data processing workflow.

Due to its history, cryo-ET has also inherited many tools and conventions from cryo-EM, even in cases where they had to be stretched thin in order to fit the needs of the new technique.
The existing de facto standard is arguably set by Relion, which in recent years has added explicit support for cryo-ET preprocessing and STA, encoding reconstruction and particles metadata in its STAR file format, which was previously only used for single particle analysis~\cite{zivanovBayesianApproachSingleparticle2022,burtImageProcessingPipeline2024}.
Several other formats exist for particle, alignment, and various other parameters and tilt-series metadata (Dynamo tables~\cite{castano-diezDynamoCatalogueGeometrical2017}, AreTomo alignments~\cite{zhengAreTomoIntegratedSoftware2022}, Warp spline grids~\cite{tegunovRealtimeCryoelectronMicroscopy2019}, etc.), and while some of them can be interchangeable, others are virtually impossible to convert between.

Currently, there are several attempts at intercommunication between software through converters and pipeline managers.
Scipion~\cite{delarosa-trevinScipionSoftwareFramework2016} is likely the most effective and prolific, but it fights an uphill battle, integrating wildly different software, languages and approaches that were never built to be collaborate.


\section{Good software practices and human-in-the-loop}

These software ecosystem issues are especially problematic for cryo-ET, where custom workflows and unique solutions are routinely required, due the diverse nature of the samples and project goals.
Developing custom tools is often unfeasible for researchers new to the technique or who lack the necessary programming skills to delve deep in the code of the existing software.
Monolithic software suites offer user-friendly interfaces and automation --- some of their stronger selling points --- which can often get in the way of custom tool injections and human intervention.
On the flip side, tools that are standalone and single-purpose provide the flexibility to be used within any workflow, but may be difficult to integrate, especially without well-established conventions.

To move away from this limiting dichotomy, the cryo-ET community needs to push for an ecosystem where automation and user-friendliness don't get in the way of customizability and control.
Such an ecosystem, however, is founded upon good software practices, which often go against the academia-machine:

\begin{enumerate}[noitemsep]
    \item build small-scoped, generalized and modular \textbf{libraries} to allow re-use by many, with a low barrier of entry
    \item develop on top of well-established generalized libraries (numpy, pandas, etc.), to minimize wheel-reinventing and lower the learning curve for new users
    \item write readable code, and document it extensively, to make adoption and contribution easier
    \item develop tools that others would want to use, not tools that only you need
    \item release code quickly and openly, without waiting for the lethargic publication industry, to encourage sharing and expedite exchange of ideas
    \item engage the community to share maintenance responsibilities and institutional knowledge, in order to prevent abandonware and ensure continued development
\end{enumerate}

These things require expertise in software development and time to dedicate to non-glamorous tasks, both of which are often missing in academia.
I was lucky enough to join the napari community --- from which I learned a lot on the former --- and to have supervisors who valued my independence and self-determination in my PhD career.

\subsection{Interactive visualization}

Other than generally good software practices, cryo-ET software would benefit from moving towards a human-in-the-loop model, where automation and human supervision are equally important.

This is exactly the goal of a workflow-agnostic and interactive visualization tool such as blik (\fullref{blik}), which allows users and developers to view, explore and manipulate the data in a seamless way at any point during the processing.

In cryo-ET, it's crucial to have powerful 3D interactivity, as 2D-only views on 3D data can be extremely limiting in understanding the system.
One visualization tool that lacked particularly when we started working on blik was a quick and interactive way to look at and modify particle poses (positions and orientations) within their context in the tomogram (though nowadays other tools with similar capabilities are available~\cite{ermelArtiaXElectronTomography2022}).

Similarly, annotation, picking and segmentation tools were (and in large part still are) limited to interactivity only in 2D; this is a limitation shared across imaging fields, which is why several napari developers are making a concerted effort to make ergonomic 3D interactivity possible.

% TODO?
% \section{A balancing act}
%
% - a case for user-friendliness (cryosparc > relion)
% - a case for open-source (relion > cryosparc)
% - a case for domain agnosticism (napari)
% - the benefits of modularity, "clean code", documentation, etc, to the research community
