\chapter[Cryo-ET software]{Cryo-ET software: state of the art}

Due to the rapid growth of cryo-ET, the software ecosystem as a whole is fragmented; conventions are either not established, or appear on a first-come-first-served basis, with little consensus or community discussion.
Meanwhile, the most established pipelines use big monolithic applications, developed with streamlined workflows in mind and little attention to code reusability.

This presents a problem, because due the diverse nature of the samples and project goals, cryo-ET typically requires custom workflows and unique solutions, which are often unfeasible for researchers new to the technique who lack the necessary programming skills to delve deep in the code of the existing software.
Additionally, software automation in cryo-ET is typically more restrictive than more streamlined techniques like SPA, because custom tool injections and human intervention are often needed at various steps of the processing.

\section{Metadata wrangling}

A common symptom of such an ecosystem is that significant time and effort is lost in dealing with metadata when setting up a cryo-ET data processing workflow.

Due to its history, cryo-ET has also inherited many tools and conventions from cryo-EM, even in cases where they had to be stretched thin in order to fit the needs of the new technique.
The existing de facto standard is arguably set by Relion, which in recent years has added explicit support for cryo-ET preprocessing and STA, encoding reconstruction and particles metadata in its STAR file format, which was previously only used for single particle analysis~\cite{zivanovBayesianApproachSingleparticle2022,burtImageProcessingPipeline2024}.
Several other formats exist for particle, alignment, and various other parameters and tilt-series metadata (Dynamo tables~\cite{castano-diezDynamoCatalogueGeometrical2017}, AreTomo alignments~\cite{zhengAreTomoIntegratedSoftware2022}, Warp spline grids~\cite{tegunovRealtimeCryoelectronMicroscopy2019}, etc.), and while some of them can be interchangeable, others are virtually impossible to convert between.

Currently, there are several attempts at intercommunication between software through converters and pipeline managers.
Scipion~\cite{delarosa-trevinScipionSoftwareFramework2016} is likely the most effective and prolific, but it fights an uphill battle, integrating wildly different software, languages and approaches that were never built to be collaborate.

\section{Monolithic vs Modular: an academia paradox}

In academia, large publications with splashy headlines can make or break career paths, rewarding speed, competition, and secrecy; unfortunately, this is not conductive to the development of good software practices.

Researchers regularly develop ad hoc solutions, lacking the time and resources to release a maintainable tool, leading to software that can hardly be adapted even to similar problems.
These researchers are also typically PhD students that will soon move to a different group, work on different projects, and won't have any more time to dedicate to their old software.
It's rarely in their best interest to keep working on software that won't give them any further publications, so there's little incentive to begin development with reusability and maintainability in mind. 
All this leads to regular wheel-reinventing, and abandonware full of great ideas but hard to reuse in any capacity for future work.

In the rare success stories where software becomes adopted widely enough to achieve long term maintenance, it's usually in the form of inscrutable monolithic applications developed by a single group or person with little community collaboration on the development.
There are of course exceptions, but for young researchers it's generally a safer bet to go the "develop and publish" route than the "design and maintain" one.

There are a few ways to go push for a different ecosystem, but they mostly go against the academia-machine:

\begin{enumerate}
    \item engage the community to share maintenance responsibilities and institutional knowledge
    \item build small-scoped, generalized and modular libraries
    \item release code quickly and openly, without waiting for the lethargic publication industry
    \item develop tools that others would want to use, not tools that only you need
\end{enumerate}

Of course, most research developers struggle with this tension, wishing for their work to be reused and improved, but unable to allocate the time and resources required.

TODO: talk about how I went about this (my particular circumstances that allowed me to be mindful of this as well). However this hole section (or even chapter?) is starting to feel more and more like Discussion material. Not sure what to do about it.

TODO: teamtomo, current goals and set up, lots of people involved ...

\section{Common pitfalls}

denoising, ai, important to look at things


\begin{outline}
\1 importance of visualisation (human-in-the-loop)
    \2 issues caused by inability to look at data (wasted work, harder to form hypotheses)
    \2 importance of interactivity (not just looking, but picking, annotating, etc). Automation is great when it works, but in cryoet it's tricky and it often doesn't
        \3 problem of automation vs customizability
\1 denoising and "prettifying"
    \2 usefulness for picking/intepreting
    \2 pitfalls (especially with rise of AI)
\1 annotation, segmentation, picking
    \2 especially hard in 3D, requires extra attention both in automated and manual procedures, and benefits especially from human-in-the-loop approach
    \2 existing software (not sure to what extent i should go here, or say to refer to the blik paper chapter)
\1 somewhere (maybe not here?) there should be a part on algorithmic/computational limitations, which are being or could be tackled in the future
\1 napari mention to lead into result chapter
\end{outline}
