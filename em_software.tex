\chapter[Cryo-ET software]{Cryo-ET software: state of the art}

Due to the rapid growth of cryo-ET, the software ecosystem as a whole is fragmented; conventions are either not established, or appear on a first-come-first-served basis, with little consensus or community discussion.
Meanwhile, the most established pipelines use big monolithic applications, developed with streamlined workflows in mind and little attention to code reusability.

This presents a problem, because due the diverse nature of the samples and project goals, cryo-ET typically requires custom workflows and unique solutions, which are often unfeasible for researchers new to the technique who lack the necessary programming skills to delve deep in the code of the existing software.
Additionally, software automation in cryo-ET is typically more restrictive than more streamlined techniques like SPA, because custom tool injections and human intervention are often needed at various steps of the processing.

\section{Metadata wrangling}

A big symptom The biggest time sink when setting up any cryo-ET data processing workflow is typically dealing with metadata.
Due to its history, cryo-ET has inherited many tools and conventions from cryo-EM, even in cases where they had to be stretched thin in order to fit the needs of the new technique.

The existing de facto standard can probably be found in Relion, which in recent years has added explicit support for cryo-ET preprocessing and STA, encoding reconstruction and particles metadata in its STAR file format~\cite{zivanovBayesianApproachSingleparticle2022,burtImageProcessingPipeline2024}.

Several other formats exist for particle, alignment, and various other parameters and metadata (Dynamo tables~\cite{castano-diezDynamoCatalogueGeometrical2017}, AreTomo alignments~\cite{zhengAreTomoIntegratedSoftware2022}, Warp XML spline grids~\cite{tegunovRealtimeCryoelectronMicroscopy2019}, etc.), and while some of them can be interchangeable, others are virtually impossible to convert between.

There are several attempts at intercommunication between software through converters and pipeline managers; Scipion~\cite{delarosa-trevinScipionSoftwareFramework2016} is arguably the most effective and prolific, despite its uphill battle at integrating wildly different software, languages and approaches.

\section{Monolithic vs Modular}



\section{Common pitfalls}

denoising, ai, important to look at things

\section{Human-in-the-loop: interactive visualisation}

In most scientific fields, the ability to visualize and interactively explore one's data is crucial to forming an understanding of the analysed system, and for hypothesis generation.
Fields working with higher dimensional data, such as cryo-ET, benefit especially from interactive visualisation, as static plots and 2D images are often unfit to capture the full scope of the system.

Unfortunately, as cryo-ET workflows become more and more automated and software tools become more abstracted from the data itself, it is easy --- and tempting --- to follow entire processing workflows while rarely looking at the data other than through summaries or final results.
This is often encouraged by the monolithic software suites currently dominating the field, making it harder to implement and use custom tools to inspect or intervene at any one point of the workflow.

This often leads to TODO


\begin{outline}
\1 importance of visualisation (human-in-the-loop)
    \2 issues caused by inability to look at data (wasted work, harder to form hypotheses)
    \2 importance of interactivity (not just looking, but picking, annotating, etc). Automation is great when it works, but in cryoet it's tricky and it often doesn't
        \3 problem of automation vs customizability
\1 denoising and "prettifying"
    \2 usefulness for picking/intepreting
    \2 pitfalls (especially with rise of AI)
\1 annotation, segmentation, picking
    \2 especially hard in 3D, requires extra attention both in automated and manual procedures, and benefits especially from human-in-the-loop approach
    \2 existing software (not sure to what extent i should go here, or say to refer to the blik paper chapter)
\1 somewhere (maybe not here?) there should be a part on algorithmic/computational limitations, which are being or could be tackled in the future
\1 napari mention to lead into result chapter
\end{outline}
